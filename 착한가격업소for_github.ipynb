{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "코랩에서 진행시 다운로드해줘야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install openai chromadb langchainhub tiktoken transformers sentence-transformers langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  전처리 과정\n",
        "중복되는 칼럼 (주소 <-> 시도,시군)을 제거하고 문맥에 좀더 중요하게끔 업소명과 가깝게 주소,업소명, 업종,  연락처 순으로 조정했습니다.\n",
        "\n",
        "새로운 data_preprocessing2.csv로 저장해서 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd \n",
        "\n",
        "# df = pd.read_csv('data.csv')\n",
        "# df2 = df.drop(['시도','시군'],axis=1)\n",
        "# new_column_order = ['주소','업소명', '업종',  '연락처', '메뉴1', '가격1', '메뉴2', '가격2', '메뉴3', '가격3']\n",
        "# df3 = df2[new_column_order]\n",
        "# df3.to_csv(\"data_preprocessing2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "data_preprocessing.csv로 저장된 파일을 랭체인의 document로더로 불러와서 사용합니다.   \n",
        "한 열씩 나눠주기위해  'delimiter': ','  ,  'quotechar': '\"' 로 나누어줍니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpOiCewJiYYb",
        "outputId": "b5f5d3a2-5988-4729-eb52-73aca006734e"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "\n",
        "loader = CSVLoader(file_path='./data_preprocessing2.csv', csv_args={\n",
        "    'delimiter': ',',\n",
        "    'quotechar': '\"',\n",
        "})\n",
        "\n",
        "data = loader.load()\n",
        "data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJwMzhH-jZLO"
      },
      "outputs": [],
      "source": [
        "OPENAI_KEY = \"openaikey\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "저장한 데이터를 벡터스토어에 저장하기위해 임베딩을 진행합니다. 임베딩 모델은 gpt의 text-embedding-3-small을 사용하여서 성능을 향상 시켰고   \n",
        "persist_directory=\"./chroma_store_with_preprocessing\" 를 통해 벡터스토어를 로컬환경에 저장할때 경로도 지정 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RmM5v7-Yji7b"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=data, embedding=OpenAIEmbeddings(openai_api_key=OPENAI_KEY,model='text-embedding-3-small'),persist_directory=\"./chroma_store_5\")\n",
        "#지정한 경로에 저장\n",
        "vectorstore.persist()\n",
        "#리트리버로 바꿔줍니다.\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "저장한 벡터스토어를 불러올때에도 모델을 명시해주어야합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "####################################\n",
        "# 저장된 Chroma 벡터 스토어 로드 \n",
        "# vectorstore =Chroma(persist_directory=\"./chroma_store_5\", embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_KEY,,model='text-embedding-3-small'))\n",
        "######################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "리트리버가 잘 작동하는지 확인합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "results = retriever.get_relevant_documents(\"대구시 음식점\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "만약 코랩에서 진행한다면 zip으로 압축해서 저장하는 것이 편합니다.\n",
        "그 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipsqn7ht6C1S",
        "outputId": "c1c79ff2-edd2-4908-c1f1-5f8ff405ef35"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from google.colab import files\n",
        "\n",
        "# # 압축할 폴더 경로 (예: ./chroma_store)\n",
        "# folder_to_zip = \"./chroma_store\"\n",
        "\n",
        "# # 압축 파일 이름\n",
        "# output_filename = \"chroma_store_1.zip\"\n",
        "\n",
        "# # 폴더 압축\n",
        "# os.system(f\"zip -r {output_filename} {folder_to_zip}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "체인으로 연결해줄 llm을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Fhi2lX18kawz"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini-2024-07-18\", temperature=0, openai_api_key=OPENAI_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZUPjLDzpi_r"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "chat봇인것을 명확하게 명시하고, 여러 발생할 수 있는 문제를 제어하기위해 prompt를 작성합니다.   \n",
        "챗봇은 과거의 대화내용을 기억하는 것이 중요하므로 ConversationBufferMemory를 이용합니다 (전체 대화내용을 기억합니다.)  \n",
        "그래도 chat_history를 프롬프트 안에 집어 넣어서 좀더 그전 맥락을 잘 파악 할 수 있도록 했습니다.\n",
        "\n",
        "ConversationalRetrievalChain을 이용해 대형언어모델,리트리버,메모리(대화내용을 기억할),프롬프트를 연결해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "nrJNF_JxvQNS"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "                     \"\"\"\n",
        "                너는 친절하고 기억력이 좋은 챗봇이며, 사용자의 질문에 정확하고 일관성 있게 답변할 수 있는 능력을 갖추고 있습니다. 다음 지침에 따라 질문에 답하십시오:\n",
        "\n",
        "                1. rag 검색 결과 활용:\n",
        "                    - 검색된 정보 중 사용자 질문에 직접 관련된 내용을 선별하여 답변에 활용하십시오.\n",
        "                    - 만약 정보가 부족하거나 모호할 경우, \"모릅니다\"라고 반드시 답변하십시오. 절대 답변을 지어내지 마십시오.\n",
        "\n",
        "                2. 지역과 가게 이름 구분:\n",
        "                    - 가게 이름에 지역 이름이 포함된 경우, 이를 가게 이름으로 우선 인식하고 해당 가게 정보를 제공합니다.\n",
        "                    - 예를 들어, '종로김밥'이라는 가게 이름이 주어졌을 때, '종로'가 지역 이름이지만 이 경우에는 가게 이름으로 처리하십시오.\n",
        "                    - 사용자가 특정 지역을 물어봤을 때, 해당 지역에 포함된 가게들을 우선적으로 추천하십시오.\n",
        "                    - 예를 들어, 사용자가 '**'를 물어본다면, '**광역시'를 포함하는 주소를 가진 가게를 추천하십시오.\n",
        "                    - 예를 들어, 사용자가 '서울'를 물어본다면, '서울특별시'를 포함하는 주소를 가진 가게를 추천하십시오.\n",
        "\n",
        "                3. 가게 추천:\n",
        "                    - 가게 이름에 지역 이름이 포함된 경우, 주소를 따로 처리하고 해당 가게가 실제로 존재하는지 확인하십시오.\n",
        "                    - 가게 이름, 주소, 메뉴, 메뉴의 가격을 제공하고, 최대한 가까운 위치의 가게를 우선 추천하십시오.\n",
        "\n",
        "                4. 가게 이름만으로 검색할 때:\n",
        "                    - 만약 가게 이름만으로 데이터를 찾지 못할 경우, 사용자가 제공한 가게 이름이 정확한지 다시 확인하십시오.\n",
        "                    - 추가 정보(예: 가게 위치 또는 메뉴 등)를 요청하여 더 정확한 검색을 시도하십시오.\n",
        "                    - 예를 들어, \"이 가게가 어느 지역에 있는지 알려주실 수 있나요?\" 또는 \"해당 가게에서 어떤 메뉴를 주문하셨는지 기억하시나요?\"와 같은 질문을 덧붙이십시오.\n",
        "\n",
        "                5. 중복된 가게가 있을 경우:\n",
        "                    - 동일한 단어가 포함된 가게가 여러 개 있을 경우, 여러 개의 가게 정보를 제공합니다.\n",
        "                    - 각 가게의 주소와 주요 메뉴를 포함하여 사용자가 선택할 수 있도록 도움을 줍니다.\n",
        "\n",
        "                6. 이전 대화 내용 기억:\n",
        "                    - 바로 이전의 대화 내용까지만 기억하십시오.\n",
        "                    - 사용자의 이전 요청 및 답변을 기억하고, 그에 맞춰 일관성 있는 답변을 제공하십시오.\n",
        "                    - 예를 들어, 특정 지역에 대한 가게 추천을 요청받은 후, 추가 추천을 요청할 경우 동일한 지역 내에서 가게를 추천하십시오.\n",
        "\n",
        "                7. 특정 가게의 정보를 물어볼 경우:\n",
        "                    - 업소명을 확인한 후 해당 가게의 정보를 제공하십시오.\n",
        "                    - 만약 업소명이 정확하지 않다면, 올바른 업소명인지 물어보고, 비슷한 이름의 가게를 제안하십시오.\n",
        "\n",
        "                8. 추가 정보 요청:\n",
        "                    - 사용자의 질문이 모호하거나 불충분한 경우, 추가 정보를 요청하여 더 정확한 답변을 제공하도록 하십시오.\n",
        "                    - 예를 들어, \"이전 대화에서 언급된 두 번째 가게가 맞습니까?\"와 같은 질문을 통해 대화의 맥락을 명확히 하십시오.\n",
        "                    - 예를 들어, 몇 번째 가게의 정보를 물으면 이전 대화 내용을 참조해서 질문을 대답하십시오.\n",
        "\n",
        "                9. 정보 누락 시 대처:\n",
        "                    - 만약 필요한 정보를 찾지 못하거나 검색 결과가 충분하지 않다면, 사용자에게 해당 정보를 제공할 수 없음을 알리고, 구체적인 추가 정보를 요청하십시오.\n",
        "\n",
        "                10. 답을 모를 경우:\n",
        "                    - \"모릅니다\"라고만 답하고, 답을 지어내지 마십시오. 모르는 것은 반드시 \"모릅니다\"라고 답해야 합니다.\n",
        "                \n",
        "                현재 대화내용 : {chat_history}\n",
        "                이전 대화의 맥락을 참고하여 질문에 정확하고 일관성 있는 답변을 제공하십시오.\n",
        "                이전 대화가 없다면 다시 질문을 해달라고 요청하십시오.\n",
        "\n",
        "                다음은 실제로 사용자에게 보여질 답변입니다:\n",
        "            \n",
        "                {context}를 활용해서 대답하십시오.\n",
        "            \"\"\"\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory, combine_docs_chain_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kasbWx97fvAL",
        "outputId": "34cf8762-0962-4456-e52c-840e288f3c94"
      },
      "outputs": [],
      "source": [
        "qa(\"대전 가게 추천좀\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa(\"두번째 가게 주소좀\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
